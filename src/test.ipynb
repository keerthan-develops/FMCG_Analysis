{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15a895e1-4d75-4fa9-aa66-e8838d46e5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ff80b89-ce51-4c27-87a2-be822216f829",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>cg-pyspark-assignment</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x14ccf5bd0>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, col, udf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import json\n",
    "# spark.sql.repl.eagerEval.enabled: Property used to format output tables better\n",
    "\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"cg-pyspark-assignment\")\n",
    "    .master(\"local\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True)\n",
    "    .getOrCreate()\n",
    "  )\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c3bfce3-b9f6-45ea-a235-76fc2d8ef6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/keerthan/Projects/FMCG_Analysis/src\n"
     ]
    }
   ],
   "source": [
    "src_path = os.getcwd()\n",
    "print(src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afde1bca-6ff2-4d71-9e46-5d5db4b90f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/keerthan/Projects/FMCG_Analysis/test'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_path = src_path.replace('src', 'test')\n",
    "test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "43c2ec93-e265-495e-ba66-07ceaca64e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/keerthan/Projects/FMCG_Analysis/test/test.json'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file = test_path + '/test.json'\n",
    "test_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "119aa439-f52f-4e6e-887c-7b9aa17681fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>Array1</th><th>Num1</th><th>Text1</th><th>Text2</th></tr>\n",
       "<tr><td>[7, 8, 9]</td><td>5.0</td><td>hello</td><td>goodbye</td></tr>\n",
       "<tr><td>[77, 88, 99]</td><td>6.6</td><td>this</td><td>that</td></tr>\n",
       "<tr><td>[555, 444, 222]</td><td>-0.03</td><td>yes</td><td>no</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---------------+-----+-----+-------+\n",
       "|         Array1| Num1|Text1|  Text2|\n",
       "+---------------+-----+-----+-------+\n",
       "|      [7, 8, 9]|  5.0|hello|goodbye|\n",
       "|   [77, 88, 99]|  6.6| this|   that|\n",
       "|[555, 444, 222]|-0.03|  yes|     no|\n",
       "+---------------+-----+-----+-------+"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_df = spark.read.json(test_file)\n",
    "simple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3540f5c3-06b3-4641-aa33-8a589209d0ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/keerthan/Projects/FMCG_Analysis/test/test2.txt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file2 = test_path + '/txt_json.txt'\n",
    "test_file2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f734c703-8988-4461-b21b-60b477513d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----+---------------------------+\n",
      "|Text1 |Text2    |Num1 |JSON1                      |\n",
      "+------+---------+-----+---------------------------+\n",
      "|hello | goodbye |5.0  | {\"Sub1\":\"john\", \"Sub2\":3} |\n",
      "|this  | that    |6.6  | {\"Sub1\":\"betty\", \"Sub2\":4}|\n",
      "|yes   | no      |-0.03| {\"Sub1\":\"bobby\", \"Sub2\":5}|\n",
      "+------+---------+-----+---------------------------+\n",
      "\n",
      "root\n",
      " |-- Text1: string (nullable = true)\n",
      " |-- Text2: string (nullable = true)\n",
      " |-- Num1: double (nullable = true)\n",
      " |-- JSON1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "txt_jsondf = spark.read.option('inferSchema', 'True').option('header', 'True').option('delimiter', '|').csv(test_file2)\n",
    "txt_jsondf.show(100, 0)\n",
    "txt_jsondf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a2c5785-b2bd-4242-99c7-afee4e501044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema of the JSON string.\n",
    "schema = StructType([\n",
    "  StructField(\"Sub1\", StringType()), \n",
    "  StructField(\"Sub2\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "24eced0c-91e8-4543-9db4-b4abba061fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----+---------------------------+----------+\n",
      "|Text1 |Text2    |Num1 |JSON1                      |from_json |\n",
      "+------+---------+-----+---------------------------+----------+\n",
      "|hello | goodbye |5.0  | {\"Sub1\":\"john\", \"Sub2\":3} |{john, 3} |\n",
      "|this  | that    |6.6  | {\"Sub1\":\"betty\", \"Sub2\":4}|{betty, 4}|\n",
      "|yes   | no      |-0.03| {\"Sub1\":\"bobby\", \"Sub2\":5}|{bobby, 5}|\n",
      "+------+---------+-----+---------------------------+----------+\n",
      "\n",
      "root\n",
      " |-- Text1: string (nullable = true)\n",
      " |-- Text2: string (nullable = true)\n",
      " |-- Num1: double (nullable = true)\n",
      " |-- JSON1: string (nullable = true)\n",
      " |-- from_json: struct (nullable = true)\n",
      " |    |-- Sub1: string (nullable = true)\n",
      " |    |-- Sub2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test2DF = txt_jsondf.withColumn('from_json', from_json(col('JSON1'), schema))\n",
    "test2DF.show(100, 0)\n",
    "test2DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "048d0145-04c9-4b48-8355-b5f6081058b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----+---------------------------+----------+----------+----------+\n",
      "|Text1 |Text2    |Num1 |JSON1                      |from_json |JSON1_Sub1|JSON1_Sub2|\n",
      "+------+---------+-----+---------------------------+----------+----------+----------+\n",
      "|hello | goodbye |5.0  | {\"Sub1\":\"john\", \"Sub2\":3} |{john, 3} |john      |3         |\n",
      "|this  | that    |6.6  | {\"Sub1\":\"betty\", \"Sub2\":4}|{betty, 4}|betty     |4         |\n",
      "|yes   | no      |-0.03| {\"Sub1\":\"bobby\", \"Sub2\":5}|{bobby, 5}|bobby     |5         |\n",
      "+------+---------+-----+---------------------------+----------+----------+----------+\n",
      "\n",
      "root\n",
      " |-- Text1: string (nullable = true)\n",
      " |-- Text2: string (nullable = true)\n",
      " |-- Num1: double (nullable = true)\n",
      " |-- JSON1: string (nullable = true)\n",
      " |-- from_json: struct (nullable = true)\n",
      " |    |-- Sub1: string (nullable = true)\n",
      " |    |-- Sub2: integer (nullable = true)\n",
      " |-- JSON1_Sub1: string (nullable = true)\n",
      " |-- JSON1_Sub2: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make a separate column from one of the struct fields.\n",
    "test3DF = test2DF.withColumn(\"JSON1_Sub1\", col(\"from_json.Sub1\")).withColumn(\"JSON1_Sub2\", col(\"from_json.Sub2\"))\n",
    "test3DF.show(100, 0)\n",
    "test3DF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98a6015d-a4a6-4bb4-b226-fadfccc78c4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/keerthan/Projects/FMCG_Analysis/test/array_json.txt'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_file3 = test_path + '/array_json.txt'\n",
    "test_file3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1f02edf4-a30c-4872-860b-bc7556133c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----+---------------------------------------------------------+\n",
      "|Text1 |Text2    |Num1 |JSON1                                                    |\n",
      "+------+---------+-----+---------------------------------------------------------+\n",
      "|hello | goodbye |5.0  | [{\"Sub1\":\"stop\", \"Sub2\":3}, {\"Sub1\":\"go\", \"Sub2\":6}]    |\n",
      "|this  | that    |6.6  | [{\"Sub1\":\"eggs\", \"Sub2\":4}, {\"Sub1\":\"bacon\", \"Sub2\":8}] |\n",
      "|yes   | no      |-0.03| [{\"Sub1\":\"apple\", \"Sub2\":5}, {\"Sub1\":\"pear\", \"Sub2\":10}]|\n",
      "+------+---------+-----+---------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- Text1: string (nullable = true)\n",
      " |-- Text2: string (nullable = true)\n",
      " |-- Num1: double (nullable = true)\n",
      " |-- JSON1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "array_jsondf = spark.read.options(header='True', inferSchema='True', delimiter='|').csv(test_file3)\n",
    "array_jsondf.show(100, 0)\n",
    "array_jsondf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "72169481-83d8-473b-a9cb-1ca34d243f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "array_json_schema = StructType([\n",
    "    StructField('Sub1', StringType()),\n",
    "    StructField('Sub2', IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6c4a9a08-76e4-4f1c-bc80-a096c0a0fdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(array_str):\n",
    "    listofdict = json.loads(array_str)\n",
    "    for i in listofdict:\n",
    "        yield (i['Sub1'], i['Sub2'])\n",
    "\n",
    "parse_json_udf = udf(parse_json, array_json_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "09700b12-8278-403b-8c99-ecd01fb0598e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 00:29:47 ERROR Executor: Exception in task 0.0 in stage 436.0 (TID 436)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1072, in toInternal\n",
      "    raise PySparkValueError(\n",
      "pyspark.errors.exceptions.base.PySparkValueError: [UNEXPECTED_TUPLE_WITH_STRUCT] Unexpected tuple <generator object parse_json at 0x103565d20> with StructType.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "24/03/28 00:29:47 WARN TaskSetManager: Lost task 0.0 in stage 436.0 (TID 436) (192.168.1.15 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n",
      "    serializer.dump_stream(out_iter, outfile)\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n",
      "    self.serializer.dump_stream(self._batched(iterator), stream)\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n",
      "    for obj in iterator:\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n",
      "    for item in iterator:\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in mapper\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in <genexpr>\n",
      "    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n",
      "    return lambda *a: toInternal(f(*a))\n",
      "                      ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1072, in toInternal\n",
      "    raise PySparkValueError(\n",
      "pyspark.errors.exceptions.base.PySparkValueError: [UNEXPECTED_TUPLE_WITH_STRUCT] Unexpected tuple <generator object parse_json at 0x103565d20> with StructType.\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n",
      "24/03/28 00:29:47 ERROR TaskSetManager: Task 0 in stage 436.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n    for item in iterator:\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    return lambda *a: toInternal(f(*a))\n                      ^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1072, in toInternal\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [UNEXPECTED_TUPLE_WITH_STRUCT] Unexpected tuple <generator object parse_json at 0x103565d20> with StructType.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m array_jsondf2 \u001b[38;5;241m=\u001b[39m array_jsondf\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparse_json\u001b[39m\u001b[38;5;124m'\u001b[39m, parse_json_udf(col(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mJSON1\u001b[39m\u001b[38;5;124m'\u001b[39m)))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(array_jsondf2))\n\u001b[0;32m----> 3\u001b[0m array_jsondf2\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m10\u001b[39m,\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/sql/dataframe.py:976\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    969\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    970\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m         },\n\u001b[1;32m    974\u001b[0m     )\n\u001b[0;32m--> 976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, int_truncate, vertical)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n    process()\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1239, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 225, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 146, in dump_stream\n    for obj in iterator:\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/serializers.py\", line 214, in _batched\n    for item in iterator:\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1070, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 104, in <lambda>\n    return lambda *a: toInternal(f(*a))\n                      ^^^^^^^^^^^^^^^^^\n  File \"/opt/homebrew/anaconda3/lib/python3.11/site-packages/pyspark/python/lib/pyspark.zip/pyspark/sql/types.py\", line 1072, in toInternal\n    raise PySparkValueError(\npyspark.errors.exceptions.base.PySparkValueError: [UNEXPECTED_TUPLE_WITH_STRUCT] Unexpected tuple <generator object parse_json at 0x103565d20> with StructType.\n"
     ]
    }
   ],
   "source": [
    "array_jsondf2 = array_jsondf.withColumn('parse_json', parse_json_udf(col('JSON1')))\n",
    "print(type(array_jsondf2))\n",
    "array_jsondf2.show(10,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c76a572-fd67-46ec-b891-81c757d56931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Name': 'Jennifer Smith', 'Contact Number': 7867567898, 'Email': 'jen123@gmail.com', 'Hobbies': ['Reading', 'Sketching', 'Horse Riding']}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "x = \"\"\"{\n",
    "    \"Name\": \"Jennifer Smith\",\n",
    "    \"Contact Number\": 7867567898,\n",
    "    \"Email\": \"jen123@gmail.com\",\n",
    "    \"Hobbies\":[\"Reading\", \"Sketching\", \"Horse Riding\"]\n",
    "    }\"\"\"\n",
    "y = json.loads(x)\n",
    "print(y)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7bef5635-a036-469a-9622-339aa05dd1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{\"Sub1\":\"stop\", \"Sub2\":3}, {\"Sub1\":\"go\", \"Sub2\":6}]\n",
      "<class 'str'>\n",
      "[{'Sub1': 'stop', 'Sub2': 3}, {'Sub1': 'go', 'Sub2': 6}]\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "a =  \"\"\"[{\"Sub1\":\"stop\", \"Sub2\":3}, {\"Sub1\":\"go\", \"Sub2\":6}]\"\"\"\n",
    "print(a)\n",
    "print(type(a))\n",
    "b = json.loads(a)\n",
    "print(b)\n",
    "print(type(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "49c082d9-0ca0-451c-865d-62a3a7864339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for i in b:\n",
    "    print(type(i))\n",
    "    #row = json.loads(i)\n",
    "    #print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b082c9-6145-4ace-b98d-b3232a6568fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_fmcg",
   "language": "python",
   "name": "pyspark_fmcg"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
